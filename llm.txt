# main.py

from fastapi import FastAPI, UploadFile, File, HTTPException, status
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, ValidationError
from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import List, Optional, Dict, Any
import os
import shutil
import magic
from pathlib import Path
import logging
import re
import json
import uuid

# Document Loaders for text extraction
from PyPDF2 import PdfReader

# LangChain components for text splitting, embeddings, vector store
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import Chroma

# Hugging Face Transformers for local LLM
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch # Required for PyTorch backend

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Pydantic Schemas for Syllabus Structure Output ---
# These do NOT need to inherit from LangChain's LCBaseModel anymore
# as we are handling structured output directly.
class SubUnit(BaseModel):
    title: str = Field(description="The title of the sub-unit, e.g., '1.1 Types', '2.3 Interprocess Communication'.")
    content: str = Field(description="The full textual content of this sub-unit.")

class Unit(BaseModel):
    title: str = Field(description="The title of the main unit, e.g., 'Unit 1: Introduction', 'Chapter 2: Processes'.")
    content: str = Field(description="The full textual content of this main unit, excluding content explicitly within its sub-units. Can be empty if all content is within sub-units.")
    sub_units: List[SubUnit] = Field(default_factory=list, description="A list of sub-units within this main unit.")

class SyllabusStructure(BaseModel):
    units_data: List[Unit] = Field(description="A list of main units found in the syllabus, each with its title, content, and nested sub-units.")


# --- Pydantic Schemas for Generated Notes Output ---
class GeneratedNote(BaseModel):
    topicTitle: str = Field(description="The main topic/chapter title this note relates to (e.g., 'Introduction to React').")
    title: str = Field(description="A concise title for the generated resource/note (e.g., 'Syllabus Excerpt for React Basics').")
    snippet: str = Field(description="A brief summary or excerpt of the resource content, taken directly from the syllabus.")
    url: str = Field(description="A placeholder or dummy URL for the resource.")
    source: str = Field(description="The type of source (e.g., 'syllabus_excerpt').")

class GeneratedNotesList(BaseModel):
    notes: List[GeneratedNote] = Field(description="A list of generated notes/resources for the given topics.")

# --- Pydantic Schema for Generate Notes Request ---
class GenerateNotesRequest(BaseModel):
    topics: List[str] = Field(description="List of topic titles for which to generate notes.")
    syllabus_id: str = Field(description="The unique ID of the syllabus to retrieve context from.")


# --- Settings Management ---
class Settings(BaseSettings):
    chroma_db_path: str = "./chroma_db"
    supported_file_types: List[str] = ["application/pdf"]
    upload_dir: str = "uploaded_files"
    
    # --- Hugging Face LLM Specific Settings ---
    # Using 'microsoft/phi-2' is a good balance for 8GB M1 Air
    # You can try 'mistralai/Mistral-7B-Instruct-v0.2' for more power, but it will be slower on 8GB.
    hf_model_id: str = "microsoft/phi-2" # Or "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    hf_cache_dir: str = "./hf_models_cache" # Directory to store downloaded HF models

    model_config = SettingsConfigDict(env_file=".env", extra="ignore") # .env still used for other settings if present

settings = Settings()

# Ensure directories exist
try:
    Path(settings.upload_dir).mkdir(parents=True, exist_ok=True)
    Path(settings.chroma_db_path).mkdir(parents=True, exist_ok=True)
    Path(settings.hf_cache_dir).mkdir(parents=True, exist_ok=True) # Create HF cache dir
    logger.info(f"Ensured '{settings.upload_dir}', '{settings.chroma_db_path}', and '{settings.hf_cache_dir}' directories exist.")
except OSError as e:
    logger.critical(f"Failed to create necessary directories: {e}. Please check permissions.")
    exit(1)

# --- LangChain Components Initialization ---
embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
logger.info("SentenceTransformerEmbeddings model 'all-MiniLM-L6-v2' initialized.")

vector_store = Chroma(persist_directory=settings.chroma_db_path, embedding_function=embeddings)
logger.info(f"ChromaDB initialized, persistent directory: {settings.chroma_db_path}")

# --- Local LLM Initialization (Hugging Face) ---
hf_model = None
hf_tokenizer = None

try:
    # Check for Apple Silicon (MPS) device
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        logger.info("Using Apple Silicon (MPS) for Hugging Face model inference.")
    else:
        device = torch.device("cpu")
        logger.warning("Apple Silicon (MPS) not available. Falling back to CPU for Hugging Face model inference. This will be slower.")

    logger.info(f"Loading Hugging Face model: {settings.hf_model_id} from cache: {settings.hf_cache_dir}")
    
    # Load tokenizer
    hf_tokenizer = AutoTokenizer.from_pretrained(settings.hf_model_id, cache_dir=settings.hf_cache_dir)
    
    # Load model. Use torch_dtype=torch.float16 for memory efficiency on MPS.
    hf_model = AutoModelForCausalLM.from_pretrained(
        settings.hf_model_id, 
        trust_remote_code=True, 
        torch_dtype=torch.float16 if device.type == "mps" else torch.float32, 
        cache_dir=settings.hf_cache_dir
    ).to(device)
    
    hf_model.eval() # Set model to evaluation mode
    logger.info(f"Hugging Face model '{settings.hf_model_id}' loaded successfully on {device}.")

except Exception as e:
    logger.critical(f"Failed to load Hugging Face model '{settings.hf_model_id}': {e}. Ensure enough RAM and correct dependencies.", exc_info=True)
    # Exit if LLM cannot be loaded, as it's critical for syllabus extraction
    exit(1)


# --- FastAPI App Initialization ---
app = FastAPI(
    title="NoteMate Backend - Syllabus & Notes Processor (Local HF LLM)",
    description="API for uploading and processing PDF syllabuses, and generating study notes from selected chapters using a local Hugging Face LLM and RAG.",
    version="0.1.0"
)

# Configure CORS to allow requests from your React frontend
origins = [
    "http://localhost",
    "http://localhost:3000",
    "http://127.0.0.1",
    "http://127.0.0.1:3000",
    "http://localhost:5173",
    "http://127.0.0.1:5173",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Helper Functions for Document Processing and Syllabus Extraction ---

def get_file_type(file_path: str) -> str:
    """Detects the MIME type of a file."""
    return magic.from_file(file_path, mime=True)

def extract_text_from_pdf(pdf_path: str) -> str:
    """Extracts text content from a PDF file."""
    text = ""
    try:
        with open(pdf_path, "rb") as file:
            reader = PdfReader(file)
            for page in reader.pages:
                text += page.extract_text() or ""
        if not text.strip():
            logger.warning(f"No meaningful text extracted from PDF: {pdf_path}")
    except Exception as e:
        logger.error(f"Error extracting text from PDF '{pdf_path}': {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error extracting text from PDF: {e}. Ensure it's not encrypted or corrupted.")
    return text


def extract_syllabus_structure_llm(syllabus_text: str) -> SyllabusStructure:
    """
    Extracts units and sub-units from syllabus text using a local Hugging Face LLM.
    """
    if hf_model is None or hf_tokenizer is None:
        raise HTTPException(
            status_code=500,
            detail="Local Hugging Face LLM not initialized. Check server logs."
        )

    # --- Prompt Engineering for Structured Output ---
    # We guide the LLM to produce JSON that matches our Pydantic schema
    prompt_template = f"""You are an expert academic assistant tasked with extracting structured syllabus information.
Your goal is to parse the provided text and accurately identify main units/chapters and their sub-units.
For each unit and sub-unit, extract its title and all its associated content.
Content for a unit should include introductory text before its first sub-unit.
Content for a sub-unit should include the heading line itself and all text following it until the next sub-unit or main unit.
If a unit has no explicit sub-units, put all its content directly into the unit's content field.
Always prioritize capturing all relevant text into the content fields.

The output MUST be a JSON object conforming to the following structure:
{{
  "units_data": [
    {{
      "title": "...",
      "content": "...",
      "sub_units": [
        {{
          "title": "...",
          "content": "..."
        }},
        {{
          "title": "...",
          "content": "..."
        }}
      ]
    }}
  ]
}}

Ensure all strings are properly escaped for JSON. Do not include any additional text or markdown outside the JSON object.

Syllabus Text:JSON Output:
"""
    
    # Tokenize the prompt
    inputs = hf_tokenizer(prompt_template, return_tensors="pt", max_length=2048, truncation=True)
    # Move inputs to the correct device (CPU/MPS)
    for k, v in inputs.items():
        inputs[k] = v.to(hf_model.device)

    try:
        # Generate output from the model
        # max_new_tokens: Adjust based on expected output size and performance. 
        # temperature: Lower temperature for more deterministic, factual output.
        # do_sample: Set to False for greedy decoding, True for sampling.
        # pad_token_id: Required for some models when using batching, often equals eos_token_id.
        output_tokens = hf_model.generate(
            **inputs,
            max_new_tokens=2048, # Max tokens the model will generate
            do_sample=False,    # Use greedy decoding for structured output
            temperature=0.1,    # Low temperature for factual extraction
            pad_token_id=hf_tokenizer.eos_token_id # Prevents infinite generation if model generates EOS early
        )
        
        # Decode the generated tokens
        # We need to slice to get only the new tokens generated by the model
        # Find the start of the generated output (after the prompt)
        generated_text_full = hf_tokenizer.decode(output_tokens[0], skip_special_tokens=True)
        
        # Slice to get only the newly generated part (after the prompt)
        # This assumes the model echoes the prompt, which is common.
        if generated_text_full.startswith(prompt_template):
            generated_text = generated_text_full[len(prompt_template):].strip()
        else:
            generated_text = generated_text_full.strip() # Fallback if prompt is not echoed

        # Sometimes models wrap JSON in markdown code blocks
        if generated_text.startswith("```json"):
            generated_text = generated_text[len("```json"):].strip()
        if generated_text.endswith("```"):
            generated_text = generated_text[:-len("```")].strip()

        # Attempt to parse the JSON output
        parsed_data = json.loads(generated_text)
        
        # Validate the parsed data against our Pydantic schema
        syllabus_structure = SyllabusStructure(**parsed_data)
        
        logger.info(f"LLM-based extraction successful. Found {len(syllabus_structure.units_data)} main units.")
        return syllabus_structure
    except json.JSONDecodeError as e:
        logger.error(f"Failed to decode JSON from LLM output: {e}. Raw output: \n{generated_text}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"LLM did not produce valid JSON output. Please try again or check model compatibility. Error: {e}"
        )
    except ValidationError as e:
        logger.error(f"LLM output validation failed: {e}. Raw output: \n{generated_text}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"LLM provided output that could not be parsed into the expected syllabus structure. Validation error: {e.errors()}"
        )
    except Exception as e:
        logger.error(f"Error during LLM-based syllabus extraction: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"An error occurred during LLM-based syllabus extraction: {e}. Ensure LLM is configured correctly."
        )


def process_document_and_add_to_chroma(file_path: Path, filename: str, syllabus_id: str) -> Dict[str, Any]:
    """
    Processes a document (PDF only), extracts text, splits it into chunks,
    adds to ChromaDB with associated metadata (syllabus_id), and uses LLM-based
    methods for structured unit/sub-unit extraction.
    Returns the processed data including the extracted units and the syllabus_id.
    """
    file_type = get_file_type(str(file_path))
    logger.info(f"Processing file: {filename} with detected type: {file_type} for syllabus_id: {syllabus_id}")

    if file_type not in settings.supported_file_types:
        logger.warning(f"Unsupported file type '{file_type}' uploaded: {filename}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Unsupported file type: {file_type}. Only PDF files are supported."
        )

    raw_text = extract_text_from_pdf(str(file_path))

    if not raw_text.strip():
        logger.warning(f"Extracted no meaningful text from {filename} (syllabus_id: {syllabus_id}). Skipping further processing.")
        return {"message": "File processed, but no significant text was extracted.", "filename": filename, "units_data": [], "syllabus_id": syllabus_id}

    logger.info(f"Raw text extracted from {filename} (first 500 chars):\n{raw_text[:500]}...")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
        is_separator_regex=False,
    )
    texts = text_splitter.split_text(raw_text)
    logger.info(f"Split {filename} into {len(texts)} chunks for vector store.")

    try:
        metadatas = [{"syllabus_id": syllabus_id, "source_filename": filename, "chunk_index": i} for i, _ in enumerate(texts)]
        
        vector_store.add_texts(texts, metadatas=metadatas)
        vector_store.persist()
        logger.info(f"Successfully added {len(texts)} chunks from {filename} to ChromaDB with syllabus_id: {syllabus_id}.")
    except Exception as e:
        logger.error(f"Error adding chunks to ChromaDB for {filename} (syllabus_id: {syllabus_id}): {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error processing document for knowledge base: {e}. Check ChromaDB setup or permissions.")

    try:
        # --- Using the Local LLM-based extraction ---
        syllabus_structure_obj = extract_syllabus_structure_llm(raw_text)
        parsed_units_data = syllabus_structure_obj.units_data
        logger.info(f"Local LLM-based extraction identified {len(parsed_units_data)} main units from {filename} (syllabus_id: {syllabus_id}).")
        
        return {
            "message": "File processed, added to knowledge base, and syllabus structure extracted.",
            "filename": filename,
            "chunks_added": len(texts),
            "units_data": [unit.model_dump() for unit in parsed_units_data],
            "syllabus_id": syllabus_id
        }
    except HTTPException as e:
        raise e # Re-raise HTTPExceptions from extract_syllabus_structure_llm
    except Exception as e:
        logger.error(f"Failed to extract syllabus structure using LLM for {filename} (syllabus_id: {syllabus_id}): {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to extract syllabus structure using LLM: {e}. Ensure LLM is configured correctly.")


# --- API Endpoints ---

@app.post("/process-syllabus/", summary="Upload and process a syllabus (PDF) to build the knowledge base and extract chapters.")
async def process_syllabus(file: UploadFile = File(...)):
    """
    Handles the upload of a syllabus PDF file. It extracts text, chunks it,
    adds it to the ChromaDB knowledge base with a unique `syllabus_id`,
    and then uses a local Hugging Face LLM to identify main units and their nested sub-units.
    The `syllabus_id` is returned, which is critical for subsequent note generation
    requests to retrieve context from the correct syllabus.
    """
    if file.content_type != "application/pdf":
        logger.warning(f"Rejected upload: Unsupported file type '{file.content_type}' for '{file.filename}'.")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Unsupported file type: {file.content_type}. Please upload a PDF file."
        )

    current_syllabus_id = str(uuid.uuid4())
    file_location = Path(settings.upload_dir) / f"{current_syllabus_id}_{file.filename}"
    
    try:
        with open(file_location, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        logger.info(f"File '{file.filename}' saved temporarily to '{file_location}' for syllabus_id: {current_syllabus_id}")

        result = process_document_and_add_to_chroma(file_location, file.filename, current_syllabus_id)
        
        return JSONResponse(status_code=status.HTTP_200_OK, content=result)
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"Critical error during syllabus processing for '{file.filename}' (syllabus_id: {current_syllabus_id}): {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to process syllabus: An internal server error occurred. {e}")
    finally:
        if file_location.exists():
            os.remove(file_location)
            logger.info(f"Temporary file '{file_location.name}' (syllabus_id: {current_syllabus_id}) removed.")

@app.post("/generate-notes/", response_model=GeneratedNotesList, summary="Generate study notes for selected topics using local RAG (no AI synthesis).")
async def generate_notes(request: GenerateNotesRequest):
    """
    Generates study notes based on relevant context retrieved from ChromaDB.
    This version uses only local RAG (Retrieval Augmented Generation) by finding
    relevant text chunks and presenting them directly, without external AI synthesis.
    """
    topics = request.topics
    syllabus_id = request.syllabus_id

    if not topics:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="No topics provided for note generation.")
    if not syllabus_id:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Syllabus ID is required for note generation.")
    
    all_generated_notes = []
    
    for topic in topics:
        try:
            logger.info(f"Retrieving context for topic: '{topic}' from ChromaDB with syllabus_id: {syllabus_id}.")
            retrieved_docs = vector_store.similarity_search(
                query=topic,
                k=5, # Get top 5 relevant chunks
                where={"syllabus_id": syllabus_id}
            )
            
            context_content = []
            for doc in retrieved_docs:
                context_content.append(doc.page_content.strip())
            
            full_context_text = "\n\n---\n\n".join(context_content)
            
            if not full_context_text.strip():
                logger.warning(f"No relevant context found in ChromaDB for topic: '{topic}' and syllabus_id '{syllabus_id}'. Generating a general placeholder note.")
                generated_note = GeneratedNote(
                    topicTitle=topic,
                    title=f"No Syllabus Context Found for {topic}",
                    snippet=f"Unable to find specific content for '{topic}' in the uploaded syllabus. This note provides general information only.",
                    url=f"https://example.com/general-info/{topic.replace(' ', '-').lower()}",
                    source="general"
                )
            else:
                logger.info(f"Generating note for topic: '{topic}' from retrieved syllabus context (no AI synthesis).")
                generated_note = GeneratedNote(
                    topicTitle=topic,
                    title=f"Syllabus Excerpt: {topic}",
                    snippet=full_context_text,
                    url=f"https://example.com/syllabus-context/{topic.replace(' ', '-').lower()}",
                    source="syllabus_excerpt"
                )

            all_generated_notes.append(generated_note)

        except Exception as e:
            logger.error(f"Unexpected error during note generation for topic '{topic}' (syllabus_id: {syllabus_id}): {e}", exc_info=True)
            all_generated_notes.append(GeneratedNote(
                topicTitle=topic,
                title=f"Error generating note for {topic}",
                snippet=f"An unexpected error occurred during note retrieval: {e}. Please try again.",
                url="#error",
                source="error"
            ))
            
    return GeneratedNotesList(notes=all_generated_notes)


@app.get("/", include_in_schema=False)
async def read_root():
    """Root endpoint, redirects to API docs."""
    return {"message": "NoteMate Backend is running. Access /docs for API documentation."}